"use strict";(self.webpackChunkmare_ai=self.webpackChunkmare_ai||[]).push([[935],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>c});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),m=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=m(e.components);return a.createElement(s.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=m(t),c=r,g=u["".concat(s,".").concat(c)]||u[c]||d[c]||i;return t?a.createElement(g,o(o({ref:n},p),{},{components:t})):a.createElement(g,o({ref:n},p))}));function c(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var m=2;m<i;m++)o[m]=t[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},7568:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>m});var a=t(7462),r=(t(7294),t(3905));const i={sidebar_position:1},o="Named entity recognition",l={unversionedId:"models/named-entity-recognition",id:"models/named-entity-recognition",title:"Named entity recognition",description:"Extracting Named Entity Recognition / Key Value pair extraction",source:"@site/docs/models/named-entity-recognition.md",sourceDirName:"models",slug:"/models/named-entity-recognition",permalink:"/docs/models/named-entity-recognition",draft:!1,editUrl:"https://github.com/gregbugaj/marie-ai/tree/develop/docs/docs/models/named-entity-recognition.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Model Zoo",permalink:"/docs/models/model-zoo"},next:{title:"Registry",permalink:"/docs/models/model-registry"}},s={},m=[{value:"Configuration",id:"configuration",level:2},{value:"Examples",id:"examples",level:2},{value:"Executor setup",id:"executor-setup",level:3},{value:"Complete NER example",id:"complete-ner-example",level:3},{value:"Fine-Tuning LayoutLM v3",id:"fine-tuning-layoutlm-v3",level:2},{value:"Setup Development Environment",id:"setup-development-environment",level:3},{value:"Load and prepare dataset",id:"load-and-prepare-dataset",level:3},{value:"Directory structure",id:"directory-structure",level:4},{value:"Utility usage",id:"utility-usage",level:4},{value:"command : convert",id:"command--convert",level:4},{value:"command : decorate",id:"command--decorate",level:4},{value:"command : augment",id:"command--augment",level:4},{value:"command : rescale",id:"command--rescale",level:4},{value:"command : convert-all",id:"command--convert-all",level:4},{value:"command : visualize",id:"command--visualize",level:4},{value:"command : split",id:"command--split",level:4},{value:"Configuration",id:"configuration-1",level:4},{value:"Duplicate Mapping",id:"duplicate-mapping",level:4},{value:"Key-Value validation",id:"key-value-validation",level:4},{value:"Linking Fields",id:"linking-fields",level:4},{value:"Training",id:"training",level:3},{value:"Reference",id:"reference",level:2}],p={toc:m};function d(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"named-entity-recognition"},"Named entity recognition"),(0,r.kt)("p",null,"Extracting Named Entity Recognition / Key Value pair extraction"),(0,r.kt)("h2",{id:"configuration"},"Configuration"),(0,r.kt)("h2",{id:"examples"},"Examples"),(0,r.kt)("h3",{id:"executor-setup"},"Executor setup"),(0,r.kt)("p",null,"Basic executor setup and inference."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from marie.executor import NerExtractionExecutor\nfrom marie.utils.image_utils import hash_file\n\n# setup executor\nmodels_dir = "/mnt/data/models/"\nexecutor = NerExtractionExecutor(models_dir)\n\nimg_path = "/tmp/sample.png"\nchecksum = hash_file(img_path)\n\n# invoke executor\ndocs = None\nkwa = {"checksum": checksum, "img_path": img_path}\nresults = executor.extract(docs, **kwa)\n\nprint(results)\n')),(0,r.kt)("h3",{id:"complete-ner-example"},"Complete NER example"),(0,r.kt)("p",null,"Setup Named Entity Recognition executor ",(0,r.kt)("inlineCode",{parentName:"p"},"NerExtractionExecutor")," and storage backend ",(0,r.kt)("inlineCode",{parentName:"p"},"PostgreSQLStorage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import glob\nimport os\nfrom typing import Dict\n\nimport transformers\n\nfrom marie.conf.helper import storage_provider_config, load_yaml\nfrom marie.executor import NerExtractionExecutor\nfrom marie.executor.storage.PostgreSQLStorage import PostgreSQLStorage\nfrom marie.logging.profile import TimeContext\nfrom marie.registry.model_registry import ModelRegistry\nfrom marie.utils.image_utils import hash_file, hash_bytes\nfrom marie.utils.json import store_json_object\nfrom marie import (\n    Document,\n    DocumentArray,\n    __model_path__,\n    __config_dir__,\n)\n\n\ndef process_file(\n    executor: NerExtractionExecutor,\n    img_path: str,\n    storage_enabled: bool,\n    storage_conf: Dict[str, str],\n):\n    with TimeContext(f"### extraction info"):\n        filename = img_path.split("/")[-1].replace(".png", "")\n        checksum = hash_file(img_path)\n        docs = None\n        kwa = {"checksum": checksum, "img_path": img_path}\n        payload = executor.extract(docs, **kwa)\n        print(payload)\n        store_json_object(payload, f"/tmp/tensors/json/{filename}.json")\n\n        if storage_enabled:\n            storage = PostgreSQLStorage(\n                hostname=storage_conf["hostname"],\n                port=int(storage_conf["port"]),\n                username=storage_conf["username"],\n                password=storage_conf["password"],\n                database=storage_conf["database"],\n                table="check_ner_executor",\n            )\n\n            dd2 = DocumentArray([Document(content=payload)])\n            storage.add(dd2, {"ref_id": filename, "ref_type": "filename"})\n\n        return payload\n\n\ndef process_dir(\n    executor: NerExtractionExecutor,\n    image_dir: str,\n    storage_enabled: bool,\n    conf: Dict[str, str],\n):\n    for idx, img_path in enumerate(glob.glob(os.path.join(image_dir, "*.*"))):\n        try:\n            process_file(executor, img_path, storage_enabled, conf)\n        except Exception as e:\n            print(e)\n            # raise e\n\n\nif __name__ == "__main__":\n    # pip install git+https://github.com/huggingface/transformers\n    # 4.18.0  -> 4.21.0.dev0 : We should pin it to this version\n    print(transformers.__version__)\n    _name_or_path = "rms/layoutlmv3-large-corr-ner"\n    kwargs = {"__model_path__": __model_path__}\n    _name_or_path = ModelRegistry.get_local_path(_name_or_path, **kwargs)\n\n    print(__config_dir__)\n    # Load config\n    config_data = load_yaml(os.path.join(__config_dir__, "marie-debug.yml"))\n    storage_conf = storage_provider_config("postgresql", config_data)\n    executor = NerExtractionExecutor(_name_or_path)\n\n    single_file = True\n    img_path = f"/home/greg/tmp/image5839050414130576656-0.tif"\n\n    if single_file:\n        process_file(executor, img_path, True, storage_conf)\n    else:\n        process_dir(executor, img_path, True, storage_conf)\n')),(0,r.kt)("h2",{id:"fine-tuning-layoutlm-v3"},"Fine-Tuning LayoutLM v3"),(0,r.kt)("p",null,"From annotation to training and inference"),(0,r.kt)("h3",{id:"setup-development-environment"},"Setup Development Environment"),(0,r.kt)("p",null,"There are two separate environments one using ",(0,r.kt)("inlineCode",{parentName:"p"},"pip")," for Marie-AI and other using ",(0,r.kt)("inlineCode",{parentName:"p"},"conda")," for UniLM. We could mix them\nhowever there are different dependencies needed for UniLM and Marie so it is safer to keep them segregated. Additionally,\nthere is no need to use ",(0,r.kt)("inlineCode",{parentName:"p"},"conda")," as this could have been setup with ",(0,r.kt)("inlineCode",{parentName:"p"},"pip")," as well.  "),(0,r.kt)("h3",{id:"load-and-prepare-dataset"},"Load and prepare dataset"),(0,r.kt)("p",null,"Data prep is done from tools from ",(0,r.kt)("inlineCode",{parentName:"p"},"marie-ai"),", to setup development environment follow ",(0,r.kt)("a",{parentName:"p",href:"/docs/getting-started/installation"},"getting started guide"),".\nData is labeled using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/opencv/cvat"},"Computer Vision Annotation Tool (CVAT)")," in ",(0,r.kt)("a",{parentName:"p",href:"https://cocodataset.org/#format-data"},"COCO Dataset format"),"."),(0,r.kt)("p",null,"Convert CVAT annotated COCO dataset into ",(0,r.kt)("a",{parentName:"p",href:"https://guillaumejaume.github.io/FUNSD/"},"FUNSD")," compatible format for finetuning models. "),(0,r.kt)("h4",{id:"directory-structure"},"Directory structure"),(0,r.kt)("p",null,"Each directory should be in COCO 1.0 format when exporting from CVAT."),(0,r.kt)("p",null,"Example structure for ",(0,r.kt)("inlineCode",{parentName:"p"},"test")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"train")," modes, by default the data suffix of ",(0,r.kt)("inlineCode",{parentName:"p"},"-deck-raw")," will be added to the mode to\ncreate a directory name."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"~/dataset/indexer\n\u251c\u2500\u2500 test-deck-raw\n\u2502   \u251c\u2500\u2500 annotations\n\u2502   \u2502 \u2514\u2500\u2500 instances_default.json\n\u2502   \u2514\u2500\u2500 images\n\u2502       \u251c\u2500\u2500 157303757_0.png\n\u2502       \u251c\u2500\u2500 157303757_2.png\n\u2502       \u2514\u2500\u2500 157303757_4.png\n\u251c\u2500\u2500 train-deck-raw\n\u2502   \u251c\u2500\u2500 annotations\n\u2502   \u2502   \u2514\u2500\u2500 instances_default.json\n\u2502   \u2514\u2500\u2500 images\n\u2502       \u251c\u2500\u2500 157303758_0.png\n\u2502       \u251c\u2500\u2500 157303758_2.png\n\u2502       \u2514\u2500\u2500 157303758_4.png\n\u2514\u2500\u2500 validation\n")),(0,r.kt)("p",null,"Activate our marie-ai environment."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cd ~/dev/marie-ai\nsource ./venv/bin/activate\n")),(0,r.kt)("p",null,"The script performs few basic steps."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"convert   : Convert COCO to FUNSD like format "),(0,r.kt)("li",{parentName:"ul"},"decorate  : Text Box detection, ICR/OCR "),(0,r.kt)("li",{parentName:"ul"},"augment   : Data augmentation"),(0,r.kt)("li",{parentName:"ul"},"rescale   : Rescale/Normalize documents to be used by UNILM"),(0,r.kt)("li",{parentName:"ul"},"visualize : Visualize documents"),(0,r.kt)("li",{parentName:"ul"},"split     : Split COCO dataset into train/test")),(0,r.kt)("p",null,"Each command can be invoked separately, but initially they need to be invoked in following order if you don't have already\ngenerated intermediate assets."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"convert -> decorate -> augment -> rescale\n")),(0,r.kt)("h4",{id:"utility-usage"},"Utility usage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"usage: coco_funsd_converter [-h] {convert,decorate,augment,rescale,visualize,split} ...\n\nCOCO to FUNSD conversion utility\n\npositional arguments:\n  {convert,decorate,augment,rescale,visualize,split,convert-all}\n                        Commands to run\n    convert             Convert documents from COCO to FUNSD-Like intermediate format\n    decorate            Decorate documents(Box detection, ICR)\n    augment             Augment documents\n    rescale             Rescale/Normalize documents to be used by UNILM\n    visualize           Visualize documents\n    split               Split COCO dataset into train/test\n    convert-all         Run all conversion phases[convert,decorate,augment,rescale] using most defaults.\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n")),(0,r.kt)("h4",{id:"command--convert"},"command : convert"),(0,r.kt)("p",null,"Convert documents from COCO to FUNSD-Like intermediate format"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"usage: coco_funsd_converter convert [-h] --mode MODE [--mode-suffix MODE_SUFFIX] --strip_file_name_path STRIP_FILE_NAME_PATH --dir DIR [--dir_converted DIR_CONVERTED] [--dir_augmented DIR_AUGMENTED]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --mode MODE           Conversion mode : train/test/validate/etc\n  --mode-suffix MODE_SUFFIX\n                        Suffix for the mode\n  --strip_file_name_path STRIP_FILE_NAME_PATH\n                        Should full image paths be striped from annotations file\n  --dir DIR             Base data directory\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},' PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py convert --mode test \\\n --strip_file_name_path true --dir ~/dataset/private/corr-indexer \\\n --config ~/dataset/private/corr-indexer/config.json\n')),(0,r.kt)("p",null,"When we have datasets that don't line up with our annotations ",(0,r.kt)("inlineCode",{parentName:"p"},"file_image")," we can use ",(0,r.kt)("inlineCode",{parentName:"p"},"strip_file_name_path")," argument\nto strip the image path and use the image file name only."),(0,r.kt)("p",null,"Default generated folder structure will look as follows (using defaults):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u2514\u2500\u2500 test\n        \u251c\u2500\u2500 annotations_tmp    \n        \u2514\u2500\u2500 images\n")),(0,r.kt)("h4",{id:"command--decorate"},"command : decorate"),(0,r.kt)("p",null,"This step post-processed the data that have been generated via ",(0,r.kt)("inlineCode",{parentName:"p"},"convert")," command and will be created in ",(0,r.kt)("inlineCode",{parentName:"p"},"/indexer/output/dataset")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py decorate --mode test --dir ~/dataset/private/corr-indexer/output/dataset\n')),(0,r.kt)("p",null,"After the command finished output directory will have a new folder add called ",(0,r.kt)("inlineCode",{parentName:"p"},"annotations")," which contain boxes and ICR."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u2514\u2500\u2500 test\n        \u251c\u2500\u2500 annotations        <------------ \n        \u251c\u2500\u2500 annotations_tmp\n        \u2514\u2500\u2500 images\n")),(0,r.kt)("h4",{id:"command--augment"},"command : augment"),(0,r.kt)("p",null,"Augmentation is not necessary however it provides additional way to introduce variability into datasets that are small."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py augment --mode test \\\n--dir ~/dataset/private/corr-indexer/output/dataset --count 1\n')),(0,r.kt)("p",null,"After the script is run our directory structure will look as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u251c\u2500\u2500 test\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations_tmp\n    \u2502\xa0\xa0 \u2514\u2500\u2500 images\n    \u2514\u2500\u2500 test-augmented              <------------ \n        \u251c\u2500\u2500 annotations\n        \u2514\u2500\u2500 images\n")),(0,r.kt)("h4",{id:"command--rescale"},"command : rescale"),(0,r.kt)("p",null,"Augmentation is not necessary however it provides additional way to introduce variability into datasets that are small."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py rescale --mode test \\\n--dir ~/dataset/private/corr-indexer/output/dataset\n')),(0,r.kt)("p",null,"After the script is run our directory structure will look as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u251c\u2500\u2500 test\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations_tmp\n    \u2502\xa0\xa0 \u2514\u2500\u2500 images\n    \u251c\u2500\u2500 test-augmented\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations\n    \u2502\xa0\xa0 \u2514\u2500\u2500 images\n    \u2514\u2500\u2500 test-rescaled             <------------ \n        \u251c\u2500\u2500 annotations\n        \u2514\u2500\u2500 images\n")),(0,r.kt)("h4",{id:"command--convert-all"},"command : convert-all"),(0,r.kt)("p",null,"Run all conversion phases","[convert,decorate,augment,rescale]"," using most defaults. This is the fastest way to test your\nmodel and make sure that everything is configured correctly."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py convert-all --mode test  --strip_file_name_path true --aug-count 2 --dir ~/dataset/private/corr-indexer  --config ~/dataset/private/corr-indexer/config.json ```\n')),(0,r.kt)("h4",{id:"command--visualize"},"command : visualize"),(0,r.kt)("p",null,"Command for visualizing FUNDS like datasets. "),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},' PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py visualize --dir ~/dataset/private/corr-indexer/output/dataset/test-rescaled \\\n --config ~/dataset/private/corr-indexer/visualize-config.json\n')),(0,r.kt)("p",null,"Configuration is optional but if provided we will have constant label colors across images."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n "label2color": {\n  "pan": "blue",\n  "pan_answer": "green",\n  "dos": "orange"\n }\n}\n')),(0,r.kt)("h4",{id:"command--split"},"command : split"),(0,r.kt)("p",null,"Split COCO dataset for training and test."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py split --dir ~/dataset/private/corr-indexer/output/dataset/test-rescaled --ratio .8\n')),(0,r.kt)("h4",{id:"configuration-1"},"Configuration"),(0,r.kt)("p",null,"Configuration for the tool is defined via ",(0,r.kt)("inlineCode",{parentName:"p"},"--config")," attribute and file is in JSON format."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Validation")," "),(0,r.kt)("p",null,"There are couple different validations that will be performed."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Duplicate Mapping (Validation is enabled by default)"),(0,r.kt)("li",{parentName:"ul"},"Key / Value aka Question/Answer in FUNSD dataset")),(0,r.kt)("h4",{id:"duplicate-mapping"},"Duplicate Mapping"),(0,r.kt)("p",null,"Base validation to ensure that we don't have duplicate fields mappings. "),(0,r.kt)("admonition",{title:"Duplicate field check",type:"warning"},(0,r.kt)("p",{parentName:"admonition"},"Duplicate pair found for image_id","[25]"," : member_name, 4, sample.png")),(0,r.kt)("p",null,"This validation message tells us that CVAT image_id 25 (zero based) and field ",(0,r.kt)("inlineCode",{parentName:"p"},"member_name")," had 4 duplicate\nvalues present on given image."),(0,r.kt)("h4",{id:"key-value-validation"},"Key-Value validation"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"question_answer_map")," this fields maps KEY => Value, and it is one-to-one mapping, this mapping can be any field that\nwas defined in CVAT."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="Configuration fragment"',title:'"Configuration','fragment"':!0},'{\n    "question_answer_map" : {\n        "member_name": "member_name_answer",\n        "member_number": "member_number_answer",\n        "pan": "pan_answer",\n        "dos": "dos_answer",\n        "patient_name": "patient_name_answer"\n    }\n}\n')),(0,r.kt)("p",null,"When validation fails for above definition we will receive message:"),(0,r.kt)("admonition",{title:"Missing mapping",type:"warning"},(0,r.kt)("p",{parentName:"admonition"},"Missing mapping\nPair not found for image_id","[25]"," : member_name ","[1]"," MISSING -> member_name_answer ","[8]")),(0,r.kt)("p",null,"This validation message tells us that CVAT image_id 25 (zero based) and field ",(0,r.kt)("inlineCode",{parentName:"p"},"member_name")," is missing corresponding\n",(0,r.kt)("inlineCode",{parentName:"p"},"member_name_answer")," field."),(0,r.kt)("h4",{id:"linking-fields"},"Linking Fields"),(0,r.kt)("p",null,"As we are following FUNSD dataset format we have a definition for linking fields from our COCO dataset."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"id_map")," config key maps arbitrary ",(0,r.kt)("inlineCode",{parentName:"p"},"key")," to and ",(0,r.kt)("inlineCode",{parentName:"p"},"id"),".  The ",(0,r.kt)("inlineCode",{parentName:"p"},"id")," could be the same ",(0,r.kt)("inlineCode",{parentName:"p"},"id")," as used in CVAT.  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="Configuration fragment"',title:'"Configuration','fragment"':!0},'  {\n   "id_map" : {\n        "member_name": 0,\n        "member_name_answer": 1,\n        "member_number": 2,\n        "member_number_answer": 3,\n        "pan": 4,\n        "pan_answer": 5,\n        "dos": 6,\n        "dos_answer": 7,\n        "patient_name": 8,\n        "patient_name_answer": 9,\n    }\n}\n')),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"link_map")," config key maps arbitrary ",(0,r.kt)("inlineCode",{parentName:"p"},"key")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"id")," and links the two fields together."),(0,r.kt)("p",null,"This tells us that ",(0,r.kt)("inlineCode",{parentName:"p"},"member_name")," is linked to ",(0,r.kt)("inlineCode",{parentName:"p"},"[member_name, member_name_answer]")," and vice versa as the mapping is\nbidirectional."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'  {\n    "member_name": [0,1],\n    "member_name_answer": [0,1]\n  }\n')),(0,r.kt)("p",null,"To declare a field that is not linked to anything we give it a value of ",(0,r.kt)("inlineCode",{parentName:"p"},"-1")," "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'  {\n    "paragraph": [-1]\n  }\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="Configuration fragment"',title:'"Configuration','fragment"':!0},'"link_map" : {\n  "member_name": [\n    0,\n    1\n  ],\n  "member_name_answer": [\n    0,\n    1\n  ],\n  "member_number": [\n    2,\n    3\n  ],\n  "member_number_answer": [\n    2,\n    3\n  ],\n  "paragraph": [\n    -1\n  ],\n  "greeting": [\n    -1\n  ]\n}\n\n')),(0,r.kt)("h3",{id:"training"},"Training"),(0,r.kt)("p",null,"Clone UniLM: Unified pre-training for language understanding (NLU) and generation (NLG) project from following repo ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/gregbugaj/unilm.git"},"https://github.com/gregbugaj/unilm.git")," which is a fork of ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/microsoft/unilm.git"},"https://github.com/microsoft/unilm.git"),"\nFork is kept in sync, but it does contain additional changes.  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cd ~/dev\ngit clone https://github.com/gregbugaj/unilm.git\n")),(0,r.kt)("p",null,"Activate your PyTorch environment that will be used to train ",(0,r.kt)("inlineCode",{parentName:"p"},"layoutlmv3")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"conda env list\nconda activate layoutlmv3\ncd ~/dev/unilm/\n")),(0,r.kt)("p",null,"To start trainin we can use the following script "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"screen\nCUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=1 python ./train.py\n")),(0,r.kt)("h2",{id:"reference"},"Reference"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2204.08387"},"LayoutLMv3: Multi-modal Pre-training for Visually-Rich Document Understanding")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://guillaumejaume.github.io/FUNSD/"},"FUNSD: Form Understanding in Noisy Scanned Documents"))))}d.isMDXComponent=!0}}]);